{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T17:46:53.537545Z",
     "iopub.status.busy": "2024-12-22T17:46:53.537145Z",
     "iopub.status.idle": "2024-12-22T17:47:04.268986Z",
     "shell.execute_reply": "2024-12-22T17:47:04.268327Z",
     "shell.execute_reply.started": "2024-12-22T17:46:53.537511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model from Hgging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:04.270481Z",
     "iopub.status.busy": "2024-12-22T17:47:04.269967Z",
     "iopub.status.idle": "2024-12-22T17:47:04.507423Z",
     "shell.execute_reply": "2024-12-22T17:47:04.506422Z",
     "shell.execute_reply.started": "2024-12-22T17:47:04.270456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:04.509319Z",
     "iopub.status.busy": "2024-12-22T17:47:04.509072Z",
     "iopub.status.idle": "2024-12-22T17:47:10.722203Z",
     "shell.execute_reply": "2024-12-22T17:47:10.721257Z",
     "shell.execute_reply.started": "2024-12-22T17:47:04.509295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e8f6f2371541938c86ad9ebe2c7084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4a5d18d781407d812eba590397d983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512a573edfad42eb96f1f6bdd34aea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3669a23a10e04bc1b39d67ad762c9374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87d86a7c94545f6aaf5cffb64516d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67926f9f32f346278bafcd7a81f136c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79088ede693a4bb2b122ff763e245602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:12.064654Z",
     "iopub.status.busy": "2024-12-22T17:47:12.064320Z",
     "iopub.status.idle": "2024-12-22T17:47:24.249207Z",
     "shell.execute_reply": "2024-12-22T17:47:24.248616Z",
     "shell.execute_reply.started": "2024-12-22T17:47:12.064625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1dbb7d970f477cb88772a10c759324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97f8caf9034bd6a2359f56508b6ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kde4.py:   0%|          | 0.00/4.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acfdd1c978a440fbbdfacac5b35be89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72746be7ac460bb8d6a4a4c12db7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/210173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start we need to split the data into train and validation sets, but first, we see the dataset is very large so we will take only `80k` samples as our dataset and split accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:24.250333Z",
     "iopub.status.busy": "2024-12-22T17:47:24.250022Z",
     "iopub.status.idle": "2024-12-22T17:47:24.331545Z",
     "shell.execute_reply": "2024-12-22T17:47:24.330738Z",
     "shell.execute_reply.started": "2024-12-22T17:47:24.250295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 72000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 8001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 4243\n",
    "reduced_data = dataset[\"train\"].shuffle(seed=SEED).select(range(80001))\n",
    "split_dataset = reduced_data.train_test_split(train_size=0.9, seed=SEED)\n",
    "# renaming  the \"test\" key of the \"split_dataset\" as \"validation\"\n",
    "split_dataset[\"validation\"] = split_dataset.pop(\"test\")\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "1. Preparing the DatasetDict object for the model\n",
    "2. Tokenization using the model's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:24.333981Z",
     "iopub.status.busy": "2024-12-22T17:47:24.333770Z",
     "iopub.status.idle": "2024-12-22T17:47:24.341296Z",
     "shell.execute_reply": "2024-12-22T17:47:24.340484Z",
     "shell.execute_reply.started": "2024-12-22T17:47:24.333963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '121135',\n",
       " 'translation': {'en': 'Teacher text:', 'fr': 'Texte du professeur & #160;:'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:24.342851Z",
     "iopub.status.busy": "2024-12-22T17:47:24.342545Z",
     "iopub.status.idle": "2024-12-22T17:47:24.357209Z",
     "shell.execute_reply": "2024-12-22T17:47:24.356481Z",
     "shell.execute_reply.started": "2024-12-22T17:47:24.342820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define maximum length of the input sequence\n",
    "max_len = 100\n",
    "\n",
    "# define a function to tokenize the text using the model's tokenizer\n",
    "def tokenize_dataset(sentences):\n",
    "    # separate the english and french sentences into 2 lists\n",
    "    english = [sentence['en'] for sentence in sentences['translation']]\n",
    "    french = [sentence['fr'] for sentence in sentences['translation']]\n",
    "\n",
    "    # apply the tokenizer\n",
    "    inputs = tokenizer(english, text_target = french, max_length = max_len, truncation=True)\n",
    "    \n",
    "    # return tokenized inputs to be used by the model\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:24.358329Z",
     "iopub.status.busy": "2024-12-22T17:47:24.358084Z",
     "iopub.status.idle": "2024-12-22T17:47:36.039901Z",
     "shell.execute_reply": "2024-12-22T17:47:36.039037Z",
     "shell.execute_reply.started": "2024-12-22T17:47:24.358310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db4a754bbc4fe28678bbf8244c9dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d7074c715c4ae1963bbb74d3068fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the method to preprocess the dataset\n",
    "tokenized_data = split_dataset.map(function=tokenize_dataset, batched=True, remove_columns = split_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator\n",
    "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train dataset. \n",
    "So first we load it using our *tokenizer* and *model*, then we apply it on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate the model as a **TF seq2seq model** to be able to apply the collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:36.040926Z",
     "iopub.status.busy": "2024-12-22T17:47:36.040683Z",
     "iopub.status.idle": "2024-12-22T17:47:42.162193Z",
     "shell.execute_reply": "2024-12-22T17:47:42.161578Z",
     "shell.execute_reply.started": "2024-12-22T17:47:36.040905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f5f0345cb349a7b9f3ad26fd74366a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "# instantiate the model as a TF seq2seq model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:47:42.163468Z",
     "iopub.status.busy": "2024-12-22T17:47:42.163165Z",
     "iopub.status.idle": "2024-12-22T17:47:43.671646Z",
     "shell.execute_reply": "2024-12-22T17:47:43.670854Z",
     "shell.execute_reply.started": "2024-12-22T17:47:42.163437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import the collator\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# load the data collator\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model= model, return_tensors=\"tf\")\n",
    "# apply it to convert the dataset to tf.data.Dataset object\n",
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_data['train'],\n",
    "    collate_fn = collator,\n",
    "    shuffle = True,\n",
    "    batch_size = 32,\n",
    ")\n",
    "\n",
    "eval_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_data['validation'],\n",
    "    collate_fn = collator,\n",
    "    shuffle = True, \n",
    "    batch_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "Notice that we're keeping the number of epochs very low as:\n",
    "* 1st: 1 epoch can take 1h maybe 2\n",
    "* 2nd: the model learns in one epoch what a traditional model can learn in 100 (an d even more) epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T11:40:56.403744Z",
     "iopub.status.busy": "2024-12-22T11:40:56.403422Z",
     "iopub.status.idle": "2024-12-22T11:40:56.413942Z",
     "shell.execute_reply": "2024-12-22T11:40:56.413326Z",
     "shell.execute_reply.started": "2024-12-22T11:40:56.403712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "# define number of epochs\n",
    "epochs = 3\n",
    "# nbr of training steps \n",
    "train_steps = len(train_dataset)*epochs\n",
    "\n",
    "# define the optimizer\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr = 5e-5,\n",
    "    num_warmup_steps = 0,\n",
    "    num_train_steps = train_steps,\n",
    "    weight_decay_rate = 0.01,\n",
    ")\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the model we want the model to be saved on Hugging Face so we need to log to the account and specify the directory it will b saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:28:31.986879Z",
     "iopub.status.busy": "2024-12-22T10:28:31.986442Z",
     "iopub.status.idle": "2024-12-22T10:28:32.013206Z",
     "shell.execute_reply": "2024-12-22T10:28:32.012213Z",
     "shell.execute_reply.started": "2024-12-22T10:28:31.986837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ee5380bb0347ad957390cea8705c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T11:40:56.414774Z",
     "iopub.status.busy": "2024-12-22T11:40:56.414565Z",
     "iopub.status.idle": "2024-12-22T11:40:56.588041Z",
     "shell.execute_reply": "2024-12-22T11:40:56.587424Z",
     "shell.execute_reply.started": "2024-12-22T11:40:56.414755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/kaggle/working/helsinki-finetuned-en-to-fr is already a clone of https://huggingface.co/ranwakhaled/helsinki-finetuned-en-to-fr. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "callback = PushToHubCallback(output_dir=\"helsinki-finetuned-en-to-fr\",\n",
    "                             tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T11:40:56.589170Z",
     "iopub.status.busy": "2024-12-22T11:40:56.588843Z",
     "iopub.status.idle": "2024-12-22T11:40:56.758117Z",
     "shell.execute_reply": "2024-12-22T11:40:56.757074Z",
     "shell.execute_reply.started": "2024-12-22T11:40:56.589096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!git config --global user.name \"ranwakhaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T11:40:58.987802Z",
     "iopub.status.busy": "2024-12-22T11:40:58.987472Z",
     "iopub.status.idle": "2024-12-22T11:40:59.157538Z",
     "shell.execute_reply": "2024-12-22T11:40:59.156191Z",
     "shell.execute_reply.started": "2024-12-22T11:40:58.987777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! git config --global user.email \"sydneysageivashkov24@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T11:41:00.686786Z",
     "iopub.status.busy": "2024-12-22T11:41:00.686473Z",
     "iopub.status.idle": "2024-12-22T12:55:09.985105Z",
     "shell.execute_reply": "2024-12-22T12:55:09.984195Z",
     "shell.execute_reply.started": "2024-12-22T11:41:00.686761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2250/2250 [==============================] - ETA: 0s - loss: 1.1784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 1499s 657ms/step - loss: 1.1784 - val_loss: 0.9931\n",
      "Epoch 2/3\n",
      "2250/2250 [==============================] - ETA: 0s - loss: 0.8721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 1472s 654ms/step - loss: 0.8721 - val_loss: 0.9355\n",
      "Epoch 3/3\n",
      "2250/2250 [==============================] - ETA: 0s - loss: 0.7460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 1468s 652ms/step - loss: 0.7460 - val_loss: 0.9177\n"
     ]
    }
   ],
   "source": [
    "# now we train the model\n",
    "history = model.fit(train_dataset, \n",
    "                   validation_data= eval_dataset,\n",
    "                   callbacks = [callback],\n",
    "                   epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "We use the **BLEU** score available in the `sacreBLEU` library using the test set\n",
    "#### Loading the fine-tuned model to evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:06:49.408711Z",
     "iopub.status.busy": "2024-12-22T18:06:49.408354Z",
     "iopub.status.idle": "2024-12-22T18:06:51.898136Z",
     "shell.execute_reply": "2024-12-22T18:06:51.897503Z",
     "shell.execute_reply.started": "2024-12-22T18:06:49.408678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at ranwakhaled/helsinki-finetuned-en-to-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ranwakhaled/helsinki-finetuned-en-to-fr\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"ranwakhaled/helsinki-finetuned-en-to-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:07:00.746301Z",
     "iopub.status.busy": "2024-12-22T18:07:00.745980Z",
     "iopub.status.idle": "2024-12-22T18:07:05.712298Z",
     "shell.execute_reply": "2024-12-22T18:07:05.711242Z",
     "shell.execute_reply.started": "2024-12-22T18:07:00.746271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu, evaluate\n",
      "Successfully installed evaluate-0.4.3 portalocker-3.0.0 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:11:31.007156Z",
     "iopub.status.busy": "2024-12-22T18:11:31.006808Z",
     "iopub.status.idle": "2024-12-22T18:11:32.550792Z",
     "shell.execute_reply": "2024-12-22T18:11:32.549874Z",
     "shell.execute_reply.started": "2024-12-22T18:11:31.007131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89ae982c2ff46b9bc00af07ef0b263e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load the scareBLEU metric\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# create data collator for test set\n",
    "test_collator = DataCollatorForSeq2Seq(tokenizer= tokenizer,\n",
    "                                      model=model,\n",
    "                                      return_tensors = \"tf\",\n",
    "                                      pad_to_multiple_of = 128)\n",
    "# generated tensorflow dataset object\n",
    "test_dataset = model.prepare_tf_dataset(tokenized_data['validation'],\n",
    "                                       collate_fn = test_collator,\n",
    "                                       shuffle=False,\n",
    "                                       batch_size=4,)\n",
    "# shuffle dataset \n",
    "test_dataset = test_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "# take the first 200 samples\n",
    "test_dataset = test_dataset.take(200)\n",
    "\n",
    "# wrapping the fun in a @tf.function to make compilation faster\n",
    "#@tf.function(jit_compile=True)\n",
    "def generate(batch):\n",
    "    return model.generate(input_ids = batch[\"input_ids\"],\n",
    "                         attention_mask = batch[\"attention_mask\"],\n",
    "                         max_new_tokens = 128,)\n",
    "\n",
    "def compute_metrics():\n",
    "    # function to compute the bleu score for the dataset\n",
    "    all_preds = []  # list to store predictions\n",
    "    all_labels = []  # list to store labels\n",
    "\n",
    "    # using tqdm we show a progress bar for our evaluation\n",
    "    for batch, labels in tqdm(test_dataset):\n",
    "        # get the french translation (prediction)\n",
    "        predictions = generate(batch)\n",
    "        # convert tokens to words (decoding the output)\n",
    "        translations = tokenizer.batch_decode(predictions, akip_special_tokens=True)\n",
    "        # convert labels to ndarray\n",
    "        labels = labels.numpy()\n",
    "        #Replace the -100 tokens with pad_token_id (here, 59513)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # decode the labels as well\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        # strip any extra spaces resulted from post processing\n",
    "        translations = [trans.strip() for trans in translations]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "        all_preds.extend(translations)\n",
    "        all_labels.extend(decoded_labels)\n",
    "\n",
    "    # computing the BLEU metric\n",
    "    BLEU = metric.compute(predictions=all_preds, references = all_labels)\n",
    "    return BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:11:34.298101Z",
     "iopub.status.busy": "2024-12-22T18:11:34.297395Z",
     "iopub.status.idle": "2024-12-22T19:06:14.302248Z",
     "shell.execute_reply": "2024-12-22T19:06:14.301514Z",
     "shell.execute_reply.started": "2024-12-22T18:11:34.298065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [54:39<00:00, 16.40s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 4.799920346318993, 'counts': [5957, 4389, 3371, 2630], 'totals': [82496, 81696, 80896, 80096], 'precisions': [7.220956167571761, 5.3723560517038775, 4.16707871835443, 3.2835597283260087], 'bp': 1.0, 'sys_len': 82496, 'ref_len': 8178}\n"
     ]
    }
   ],
   "source": [
    "# use the compute_metrics function to evaluate the model \n",
    "metrics = compute_metrics()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on an external sentence\n",
    "Creating the `translate_sentence()` that takes a single sentence and returns the translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've noticed that the sentence is translated followed by this string  \n",
    "`Please take the official translations! You find them here: http: / /europa. eu. int/ eur-lex/ lex/ LexUriServ/ LexUriServ/ LexUriServ. do? uri=CELEX:32001L0059: EN: HTML`   \n",
    "which could be caused by the training data and could very well be eliminated if we increase training samples however since it doesn't necessarily affect the quality of the translation we'll just remove it in the post processing after the sentence is translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T19:10:18.976482Z",
     "iopub.status.busy": "2024-12-22T19:10:18.976174Z",
     "iopub.status.idle": "2024-12-22T19:10:18.981480Z",
     "shell.execute_reply": "2024-12-22T19:10:18.980523Z",
     "shell.execute_reply.started": "2024-12-22T19:10:18.976459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    batch = tokenizer([sentence], return_tensors='pt')\n",
    "    generated_ids = model.generate(**batch)\n",
    "    result = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # Remove unwanted text\n",
    "    unwanted_snippet = \"Please take the official translations! You find them here:\"\n",
    "    return result.split(unwanted_snippet)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:29.068545Z",
     "iopub.status.busy": "2024-12-22T18:02:29.068295Z",
     "iopub.status.idle": "2024-12-22T18:02:43.901050Z",
     "shell.execute_reply": "2024-12-22T18:02:43.900122Z",
     "shell.execute_reply.started": "2024-12-22T18:02:29.068523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "English: Marie Curie is the smartest scientist of her time\n",
      "Translation: Marie Curie est la scientifique la plus intelligente de son temps\n",
      "Example 1:\n",
      "English: the cat is sitting on the mat\n",
      "Translation: le chat est couché sur la grille\n",
      "Example 1:\n",
      "English: I have an exam next saturday\n",
      "Translation: J'ai un examen le samedi prochain\n",
      "Example 1:\n",
      "English: Ella needs to take the DELF  to prove her language level\n",
      "Translation: Ella doit prendre le DELF pour démontrer son niveau de langue\n",
      "Example 1:\n",
      "English: Emile Zola was one of the biggest supporters of Dreffus and was keen on proving his innocence\n",
      "Translation: Émile Zola était l'un des plus grands supporters de Dreffus et avait hâte de démontrer son innocence\n"
     ]
    }
   ],
   "source": [
    "# test the loaded model on a sample\n",
    "# Marie Curie est la savante la plus intelligente de son époque\n",
    "# le chat est assit sur le tapis\n",
    "# j'ai un controle le samedi prochain\n",
    "# Ella a besoin de passer le DELF pour prouver son niveau\n",
    "# Emile Zola était un des plus grands supporteur de Dreffus et il s'est engagé pour prouver son innocence\n",
    "\n",
    "# load the model from hugging face\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ranwakhaled/helsinki-finetuned-en-to-fr\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ranwakhaled/helsinki-finetuned-en-to-fr\")\n",
    "\n",
    "sample_text = [\"Marie Curie is the smartest scientist of her time\",\n",
    "              'the cat is sitting on the mat', \n",
    "             \"I have an exam next saturday\",\n",
    "              \"Ella needs to take the DELF  to prove her language level\",\n",
    "              \"Emile Zola was one of the biggest supporters of Dreffus and was keen on proving his innocence\"]\n",
    "for i in range(len(sample_text)):\n",
    "    print(f'Example {i}:\\nEnglish: {sample_text[i]}\\nTranslation: {translate(sample_text[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 592212,
     "sourceId": 1067156,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
